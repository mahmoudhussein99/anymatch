{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "This notebook includes all the code to preprocess datasets for the experiments in the paper. There will be two main parts: the first half is for row pairs preparation, while the second half is for attribute pairs preparation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Row Pairs Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Training & Validation Set\n",
    "\n",
    "The magellan datasets and wdc dataset need different preparation steps. We will first prepare the magellan datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/mnt/scratch/mhussein/miniconda3/envs/anymatch/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from autogluon.tabular import TabularPredictor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Magellan Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "magellan_dirs = {\n",
    "    'abt': 'raw/abt_buy', 'amgo': 'raw/amazon_google',\n",
    "    'beer': 'raw/beer', 'dbac': 'raw/dblp_acm',\n",
    "    'dbgo': 'raw/dblp_scholar', 'foza': 'raw/fodors_zagat',\n",
    "    'itam': 'raw/itunes_amazon', 'waam': 'raw/walmart_amazon',\n",
    "}\n",
    "magellan_dirs = {\n",
    "    'music': 'raw/musicbrainz20k',\n",
    "    'dbgo': 'raw/dblp_scholar', \n",
    "}\n",
    "\n",
    "magellan_rename_columns = {\n",
    "    'abt': ['id', 'name', 'description', 'price'], 'amgo': ['id', 'name', 'manufacturer', 'price'],\n",
    "    'beer': ['id', 'name', 'factory', 'style', 'ABV'], 'dbac': ['id', 'title', 'authors', 'venue', 'year'],\n",
    "    'dbgo': ['id', 'title', 'authors', 'venue', 'year'],\n",
    "    'music': ['id', 'title', 'authors', 'venue', 'year'], \n",
    "    'foza': ['id', 'name', 'address', 'city', 'phone', 'type', 'class'],\n",
    "    'itam': ['id', 'name', 'artist', 'album', 'genre', 'price', 'copyright', 'time', 'released'],\n",
    "    'waam': ['id', 'name', 'category', 'brand', 'modelno', 'price'],\n",
    "}\n",
    "\n",
    "magellan_rename_columns = {\n",
    "    'dbgo': ['id', 'title', 'authors', 'venue', 'year'],\n",
    "    'music': ['id','cluster_id','source','number', 'title', 'length', 'artist', 'album', 'year','language'], \n",
    "}\n",
    "magellan_drop_columns = {\n",
    "    'abt': ['description'], 'amgo': ['manufacturer'], 'beer': [], 'dbac': [], 'dbgo': [], 'foza': [], 'itam': [],\n",
    "    'waam': ['category', 'brand'],\n",
    "}\n",
    "magellan_drop_columns = {\n",
    "    'dbgo': ['venue','year'], \n",
    "    'music': ['cluster_id', 'source','number','length','language'],\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def merge_with_id(tableA, tableB, id_pairs):\n",
    "    left_merged = pd.merge(tableA, id_pairs, left_on='id', right_on='ltable_id')\n",
    "    left_right_merged = pd.merge(left_merged, tableB, left_on='rtable_id', right_on='id', suffixes=('_l', '_r'))\n",
    "    left_right_merged.drop(columns=['ltable_id', 'rtable_id', 'id_l', 'id_r'], inplace=True)\n",
    "    return left_right_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_magellan_row_pairs(dirs: dict, rename_columns: dict, drop_columns: dict):\n",
    "    for d_name in dirs:\n",
    "        tableA = pd.read_csv(os.path.join(dirs[d_name], 'tableA.csv'))\n",
    "        tableB = pd.read_csv(os.path.join(dirs[d_name], 'tableB.csv'))\n",
    "        tableA.columns = rename_columns[d_name]\n",
    "        tableB.columns = rename_columns[d_name]\n",
    "        tableA.drop(columns=drop_columns[d_name], inplace=True)\n",
    "        tableB.drop(columns=drop_columns[d_name], inplace=True)\n",
    "\n",
    "        train_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'train.csv'))\n",
    "        valid_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'valid.csv'))\n",
    "        test_id_pairs = pd.read_csv(os.path.join(dirs[d_name], 'test.csv'))\n",
    "        train_df = merge_with_id(tableA, tableB, train_id_pairs)\n",
    "        valid_df = merge_with_id(tableA, tableB, valid_id_pairs)\n",
    "        test_df = merge_with_id(tableA, tableB, test_id_pairs)\n",
    "\n",
    "        if not os.path.exists(f'prepared/{d_name}'):\n",
    "            os.makedirs(f'prepared/{d_name}')\n",
    "        train_df.to_csv(f'prepared/{d_name}/train.csv', index=False)\n",
    "        valid_df.to_csv(f'prepared/{d_name}/valid.csv', index=False)\n",
    "        test_df.to_csv(f'prepared/{d_name}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_magellan_row_pairs(magellan_dirs, magellan_rename_columns, magellan_drop_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### WDC Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_wdc_row_pairs(dir: str,product='computers'):\n",
    "    used_columns = ['title_left', 'price_left', 'priceCurrency_left', 'label', 'title_right', 'price_right', 'priceCurrency_right']\n",
    "    used_columns = ['title_left','description_left','brand_left', 'label',\n",
    "                    # 'price_left',\n",
    "                'title_right','description_right','brand_right',\n",
    "                # 'price_right',\n",
    "                ]\n",
    "    train_df = pd.read_csv(os.path.join(dir, 'train.csv'))[used_columns]\n",
    "    valid_df = pd.read_csv(os.path.join(dir, 'valid.csv'))[used_columns]\n",
    "    test_df = pd.read_csv(os.path.join(dir, 'test.csv'))[used_columns]\n",
    "\n",
    "    # merge_price_currency = lambda x, y: str(y) + str(x) if pd.notna(x) and pd.notna(y) else None\n",
    "    # train_df['price_left'] = train_df.apply(lambda x: merge_price_currency(x['price_left'], x['priceCurrency_left']), axis=1)\n",
    "    # train_df['price_right'] = train_df.apply(lambda x: merge_price_currency(x['price_right'], x['priceCurrency_right']), axis=1)\n",
    "    # train_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    # train_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "    train_df.columns = ['title_l', 'description_l', 'brand_l','label', 'title_r', 'description_r','brand_r']\n",
    "\n",
    "    # valid_df['price_left'] = valid_df.apply(lambda x: str(x['price_left'])+ str(x['priceCurrency_left']), axis=1)\n",
    "    # valid_df['price_right'] = valid_df.apply(lambda x: str(x['price_right'])+ str(x['priceCurrency_right']), axis=1)\n",
    "    # valid_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    # valid_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "    valid_df.columns =  ['title_l', 'description_l', 'brand_l','label', 'title_r', 'description_r','brand_r']\n",
    "\n",
    "    # test_df['price_left'] = test_df.apply(lambda x: str(x['price_left'])+ str(x['priceCurrency_left']), axis=1)\n",
    "    # test_df['price_right'] = test_df.apply(lambda x: str(x['price_right'])+ str(x['priceCurrency_right']), axis=1)\n",
    "    # test_df.drop(columns=['priceCurrency_left', 'priceCurrency_right'], inplace=True)\n",
    "    # test_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "    test_df.columns = ['title_l', 'description_l', 'brand_l','label', 'title_r', 'description_r','brand_r']\n",
    "    if not os.path.exists(f'prepared/wdc-{product}'):\n",
    "        os.makedirs(f'prepared/wdc-{product}')\n",
    "    train_df.to_csv(f'prepared/wdc-{product}/train.csv', index=False)\n",
    "    valid_df.to_csv(f'prepared/wdc-{product}/valid.csv', index=False)\n",
    "    test_df.to_csv(f'prepared/wdc-{product}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "products =['computers','watches','shoes','cameras']\n",
    "for product in products:\n",
    "    prepare_wdc_row_pairs(f'raw/wdc-{product}',product=product)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                             title_l  \\\n",
      "0  seiko men s prospex diver padi special edition...   \n",
      "1  billet egypte 25 piastres mosqu e al sayida ai...   \n",
      "2  alpina pilot quartz chronograph watch al 372lb...   \n",
      "3   darkony swatch malaysia xlite 41 45 mm yys4006ag   \n",
      "4  carl f bucherer t graph watch 00 10615 03 33 2...   \n",
      "\n",
      "                                       description_l brand_l  label  \\\n",
      "0  this gents seiko prospex diver padi special ed...     NaN      0   \n",
      "1  mosqu e al sayida aisha sign 22date mission 20...     NaN      0   \n",
      "2  p an intricate watch will always deliver sophi...     NaN      0   \n",
      "3                                                NaN     NaN      1   \n",
      "4  p at costello jewelry company in naperville il...     NaN      1   \n",
      "\n",
      "                                             title_r  \\\n",
      "0  zegarek damski le locle tissot t41 1 183 33 nu...   \n",
      "1  daniel wellington classic lady black bristol r...   \n",
      "2  alpina startimer pilot automatic watch al 725b...   \n",
      "3  darkony swatch sterreich xlite 41 45 mm yys4006ag   \n",
      "4  carl f bucherer t graph watch 00 10615 03 33 2...   \n",
      "\n",
      "                                       description_r brand_r  \n",
      "0                                                NaN  tissot  \n",
      "1                                                NaN     NaN  \n",
      "2  p an exquisite watch will always convey sophis...     NaN  \n",
      "3                                                NaN     NaN  \n",
      "4  p an intricate watch will always convey sophis...     NaN  \n"
     ]
    }
   ],
   "source": [
    "# #test wdc created correctly\n",
    "# product='watches'\n",
    "# df = pd.read_csv(f'prepared/wdc-{product}/valid.csv')\n",
    "# print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Test Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### Magellan Datasets\n",
    "The previous steps will generate a test set for each magellan dataset, while some of them will be overwritten by the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# abt_buy\n",
    "used_columns = ['name_left', 'price_left', 'label', 'name_right', 'price_right']\n",
    "renamed_columns = ['name_l', 'price_l', 'label', 'name_r', 'price_r']\n",
    "abt_df = pd.read_pickle('raw/abt_buy/test.pkl.gz')[used_columns]\n",
    "abt_df.columns = renamed_columns\n",
    "abt_df.to_csv('prepared/abt/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# amgo\n",
    "test_magellan_used_columns = {\n",
    "    'abt': ['name_left', 'price_left', 'label', 'name_right', 'price_right'],\n",
    "    'amgo': ['title_left', 'price_left', 'label', 'title_right', 'price_right'],\n",
    "    'dbac': ['title_left', 'authors_left', 'venue_left', 'year_left', 'label', 'title_right', 'authors_right', 'venue_right', 'year_right'],\n",
    "    'dbgo': ['title_left', 'authors_left', 'venue_left', 'year_left', 'label', 'title_right', 'authors_right', 'venue_right', 'year_right'],\n",
    "    'waam': ['title_left', 'modelno_left', 'price_left', 'label', 'title_right', 'modelno_right', 'price_right']\n",
    "}\n",
    "test_magellan_used_columns = {\n",
    "    'dbgo': ['title_left', 'authors_left', 'label', 'title_right', 'authors_right', ],\n",
    "    'music': ['title_left', 'artist_left', 'album_left','year_left', 'label', 'title_right', 'artist_right', 'album_right','year_right']\n",
    "}\n",
    "\n",
    "test_magellan_rename_columns = {\n",
    "    'abt': ['name_l', 'price_l', 'label', 'name_r', 'price_r'],\n",
    "    'amgo': ['name_l', 'price_l', 'label', 'name_r', 'price_r'],\n",
    "    'dbac': ['title_l', 'authors_l', 'venue_l', 'year_l', 'label', 'title_r', 'authors_r', 'venue_r', 'year_r'],\n",
    "    'dbgo': ['title_l', 'authors_l', 'venue_l', 'year_l', 'label', 'title_r', 'authors_r', 'venue_r', 'year_r'],\n",
    "    'waam': ['name_l', 'modelno_l', 'price_l', 'label', 'name_r', 'modelno_r', 'price_r']\n",
    "}\n",
    "test_magellan_rename_columns = {\n",
    "    'dbgo': ['title_l', 'authors_l', 'label', 'title_r', 'authors_r', ],\n",
    "    'music': ['title_l', 'artist_l', 'album_l', 'year_l','label', 'title_r', 'artist_r', 'album_r','year_r']\n",
    "}\n",
    "\n",
    "def prepare_test_magellan_row_pairs(dirs: dict, used_columns: dict, rename_columns: dict):\n",
    "    dirs = {key: dirs[key] for key in used_columns.keys() if key in dirs}\n",
    "    for d_name in dirs:\n",
    "        d_used_columns = used_columns[d_name]\n",
    "        d_rename_columns = rename_columns[d_name]\n",
    "        df = pd.read_csv(f'{dirs[d_name]}/test.csv')[d_used_columns]\n",
    "        df.columns = d_rename_columns\n",
    "        df.to_csv(f'prepared/{d_name}/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"['title_left', 'authors_left', 'title_right', 'authors_right'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mprepare_test_magellan_row_pairs\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmagellan_dirs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_magellan_used_columns\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_magellan_rename_columns\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 31\u001b[0m, in \u001b[0;36mprepare_test_magellan_row_pairs\u001b[0;34m(dirs, used_columns, rename_columns)\u001b[0m\n\u001b[1;32m     29\u001b[0m d_used_columns \u001b[38;5;241m=\u001b[39m used_columns[d_name]\n\u001b[1;32m     30\u001b[0m d_rename_columns \u001b[38;5;241m=\u001b[39m rename_columns[d_name]\n\u001b[0;32m---> 31\u001b[0m df \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43mf\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m{\u001b[39;49;00m\u001b[43mdirs\u001b[49m\u001b[43m[\u001b[49m\u001b[43md_name\u001b[49m\u001b[43m]\u001b[49m\u001b[38;5;132;43;01m}\u001b[39;49;00m\u001b[38;5;124;43m/test.csv\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43md_used_columns\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     32\u001b[0m df\u001b[38;5;241m.\u001b[39mcolumns \u001b[38;5;241m=\u001b[39m d_rename_columns\n\u001b[1;32m     33\u001b[0m df\u001b[38;5;241m.\u001b[39mto_csv(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprepared/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00md_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/test.csv\u001b[39m\u001b[38;5;124m'\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n",
      "File \u001b[0;32m/mnt/scratch/mhussein/miniconda3/envs/anymatch/lib/python3.9/site-packages/pandas/core/frame.py:4108\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4106\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m is_iterator(key):\n\u001b[1;32m   4107\u001b[0m         key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(key)\n\u001b[0;32m-> 4108\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_indexer_strict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcolumns\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m   4110\u001b[0m \u001b[38;5;66;03m# take() does not accept boolean indexers\u001b[39;00m\n\u001b[1;32m   4111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mgetattr\u001b[39m(indexer, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdtype\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mbool\u001b[39m:\n",
      "File \u001b[0;32m/mnt/scratch/mhussein/miniconda3/envs/anymatch/lib/python3.9/site-packages/pandas/core/indexes/base.py:6200\u001b[0m, in \u001b[0;36mIndex._get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   6197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   6198\u001b[0m     keyarr, indexer, new_indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reindex_non_unique(keyarr)\n\u001b[0;32m-> 6200\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_raise_if_missing\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkeyarr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindexer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   6202\u001b[0m keyarr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtake(indexer)\n\u001b[1;32m   6203\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(key, Index):\n\u001b[1;32m   6204\u001b[0m     \u001b[38;5;66;03m# GH 42790 - Preserve name from an Index\u001b[39;00m\n",
      "File \u001b[0;32m/mnt/scratch/mhussein/miniconda3/envs/anymatch/lib/python3.9/site-packages/pandas/core/indexes/base.py:6252\u001b[0m, in \u001b[0;36mIndex._raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   6249\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m] are in the [\u001b[39m\u001b[38;5;132;01m{\u001b[39;00maxis_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m]\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   6251\u001b[0m not_found \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(ensure_index(key)[missing_mask\u001b[38;5;241m.\u001b[39mnonzero()[\u001b[38;5;241m0\u001b[39m]]\u001b[38;5;241m.\u001b[39munique())\n\u001b[0;32m-> 6252\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnot_found\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m not in index\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['title_left', 'authors_left', 'title_right', 'authors_right'] not in index\""
     ]
    }
   ],
   "source": [
    "prepare_test_magellan_row_pairs(magellan_dirs, test_magellan_used_columns, test_magellan_rename_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "### WDC Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_test_wdc_row_pairs(dir: str):\n",
    "    used_columns = ['title_left', 'price_left', 'priceCurrency_left', 'label', 'title_right', 'price_right', 'priceCurrency_right']\n",
    "    test_df = pd.read_pickle(os.path.join(dir, 'test.pkl.gz'))[used_columns]\n",
    "\n",
    "    merge_price_currency = lambda x, y: str(y) + str(x) if pd.notna(x) and pd.notna(y) else None\n",
    "    test_df['price_left'] = test_df.apply(lambda x: merge_price_currency(x['price_left'], x['priceCurrency_left']), axis=1)\n",
    "    test_df['price_right'] = test_df.apply(lambda x: merge_price_currency(x['price_right'], x['priceCurrency_right']), axis=1)\n",
    "    test_df.drop(columns=['priceCurrency_left', 'priceCurrency_right', 'price_left', 'price_right'], inplace=True)\n",
    "    # test_df.columns = ['title_l', 'price_l', 'label', 'title_r', 'price_r']\n",
    "    test_df.columns = ['title_l', 'label', 'title_r'] # to align with the MatchGPT paper\n",
    "\n",
    "    test_df.to_csv(f'prepared/wdc/test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prepare_test_wdc_row_pairs('raw/wdc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# Attribute Pairs Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset_names = ['abt', 'amgo', 'beer', 'dbac', 'dbgo', 'foza', 'itam', 'waam', 'wdc']\n",
    "dataset_names = [f'wdc-{product}' for product in products]\n",
    "dataset_names.extend(['dbgo', 'music',])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def nan_check(value):\n",
    "    null_strings = [None, 'nan', 'NaN', 'NAN', 'null', 'NULL', 'Null', 'None', 'none', 'NONE', '', '-', '--', '---']\n",
    "    if pd.isna(value) or pd.isnull(value) or value in null_strings:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def numerical_check(value):\n",
    "    if isinstance(value, int) or isinstance(value, float):\n",
    "        return 1\n",
    "\n",
    "def string_identical_check(left_value, right_value, row_label):\n",
    "    if left_value == right_value or left_value in right_value or right_value in left_value:\n",
    "        return 1\n",
    "    else:\n",
    "        if row_label == 1:\n",
    "            return 1\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "def numerical_identical_check(left_value, right_value, row_label):\n",
    "    if left_value == right_value:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "def identical_check(left_value, right_value, row_label):\n",
    "    if nan_check(left_value) and not nan_check(right_value):\n",
    "        return 0\n",
    "    elif not nan_check(left_value) and nan_check(right_value):\n",
    "        return 0\n",
    "    elif nan_check(left_value) and nan_check(right_value):\n",
    "        return 1\n",
    "    elif numerical_check(left_value) and numerical_check(right_value):\n",
    "        return numerical_identical_check(left_value, right_value, row_label)\n",
    "    else:\n",
    "        left_value = str(left_value).lower()\n",
    "        right_value = str(right_value).lower()\n",
    "        return string_identical_check(left_value, right_value, row_label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def row2attribute_pairs(row):\n",
    "    attr_pairs = []\n",
    "    all_columns = row.index\n",
    "    left_columns = [col for col in all_columns if col.endswith('_l')]\n",
    "    right_columns = [col for col in all_columns if col.endswith('_r')]\n",
    "    row_label = row['label']\n",
    "    for i in range(len(left_columns)):\n",
    "        left_value = row[left_columns[i]]\n",
    "        right_value = row[right_columns[i]]\n",
    "        attr_pair = [left_value, right_value, identical_check(left_value, right_value, row_label), left_columns[i][:-2]]\n",
    "        attr_pairs.append(attr_pair)\n",
    "    return attr_pairs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_all_attribute_pairs(names: list):\n",
    "    for name in names:\n",
    "        train_row_pairs = pd.read_csv(f'prepared/{name}/train.csv')\n",
    "        valid_row_pairs = pd.read_csv(f'prepared/{name}/valid.csv')\n",
    "        test_row_pairs = pd.read_csv(f'prepared/{name}/test.csv')\n",
    "        train_attr_pairs = []\n",
    "        valid_attr_pairs = []\n",
    "        test_attr_pairs = []\n",
    "\n",
    "        train_row_pairs.apply(lambda row: train_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "        valid_row_pairs.apply(lambda row: valid_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "        test_row_pairs.apply(lambda row: test_attr_pairs.extend(row2attribute_pairs(row)), axis=1)\n",
    "\n",
    "        train_attr_pairs_df = pd.DataFrame(train_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        val_attr_pairs_df = pd.DataFrame(valid_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        test_attr_pairs_df = pd.DataFrame(test_attr_pairs, columns=['left_value', 'right_value', 'label', 'attribute'])\n",
    "        train_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "        val_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "        test_attr_pairs_df.drop_duplicates(inplace=True)\n",
    "\n",
    "        train_attr_pairs_df.to_csv(f'prepared/{name}/attr_train.csv', index=False)\n",
    "        val_attr_pairs_df.to_csv(f'prepared/{name}/attr_valid.csv', index=False)\n",
    "        test_attr_pairs_df.to_csv(f'prepared/{name}/attr_test.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "prepare_all_attribute_pairs(dataset_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# AutoML Predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def prepare_automl_predictions():\n",
    "    dataset_names = ['abt', 'amgo', 'beer', 'dbac', 'dbgo', 'foza', 'itam', 'waam', 'wdc']\n",
    "    dataset_names = [f'wdc-{product}' for product in products]\n",
    "    dataset_names.extend(['dbgo', 'music',])\n",
    "    dataset_names= [dataset_names[3]]\n",
    "    for name in dataset_names:\n",
    "        train_df = pd.read_csv(f'prepared/{name}/train.csv')\n",
    "        valid_df = pd.read_csv(f'prepared/{name}/valid.csv')\n",
    "\n",
    "        predictor = TabularPredictor(label='label').fit(train_data=train_df, tuning_data=valid_df, verbosity=-1)\n",
    "        train_preds = predictor.predict(train_df)\n",
    "        train_preds_proba = predictor.predict_proba(train_df)\n",
    "        valid_preds = predictor.predict(valid_df)\n",
    "        valid_preds_proba = predictor.predict_proba(valid_df)\n",
    "        train_preds_df = pd.DataFrame({'prediction': train_preds, 'proba_0': train_preds_proba[0], 'proba_1': train_preds_proba[1]})\n",
    "        valid_preds_df = pd.DataFrame({'prediction': valid_preds, 'proba_0': valid_preds_proba[0], 'proba_1': valid_preds_proba[1]})\n",
    "\n",
    "        if not os.path.exists(f'automl/{name}'):\n",
    "            os.makedirs(f'automl/{name}')\n",
    "        train_preds_df.to_csv(f'automl/{name}/train_preds.csv', index=False)\n",
    "        valid_preds_df.to_csv(f'automl/{name}/valid_preds.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No path specified. Models will be saved in: \"AutogluonModels/ag-20250204_140023\"\n"
     ]
    }
   ],
   "source": [
    "prepare_automl_predictions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "anymatch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
